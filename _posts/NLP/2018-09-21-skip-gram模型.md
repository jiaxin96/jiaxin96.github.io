---
layout: post
title: "skip-gram模型的原理和实现(1)"
date: 2018-09-22 10:05:20 +800
categories: NLP
tag: [机器学习, NLP, 向量化]
---
* content
{:toc}


# skip-gram模型的原理和实现(1)

> 本文主要说明基本的skip-gram模型，下一篇文章将会说明对skip-gram的优化和提速方法.[原论文链接](https://arxiv.org/pdf/1301.3781.pdf)


## 原理
skip-gram是一个用来压缩词向量的3层神经网络模型。训练的最终目的是获得输入层到隐藏层的权重矩阵也叫lookup table，利用矩阵的每一行作为得到的新的词向量。训练的方法是利用中心词与中心词周围窗口大小个词出现的概率大小。

### 网络结构
网络的结构如下：

![网络结构](/styles/imgs/NLP/NLP-skip-gram-struct.jpg)

完整的网络结构如下：

![完整结构](/styles/imgs/NLP/NLP-skip-gram-struct2.jpg)

一种更直观和容易理解的结构

![直观结构](/styles/imgs/NLP/skip-gram-simble-struct.jpg)

### 模型细节

* 变量维度
最初的词典大小为V，即有V个词
压缩后的词向量长度为N

* 输入层

输入层是用one-hot编码，每次输入一个中心词（长度为V的一个向量）

* 隐藏层（投影层）

分别与输入层和输出层为全联接，维度为N，其中输入层和隐藏层之间的权重矩阵为W，这是一个V*N维的矩阵，由于输入是一个one-hot向量，所以和W矩阵相乘就如同是取出了矩阵的对应一行向量，所以这个矩阵也叫查找表。运算示意图如下：

![权重矩阵](/styles/imgs/NLP/skip-matrix-in2hid.png)


* 输出层

起初看论文和文章中的模型结构的时候我对这一层也是非常的不解，但看过实现以后就知道了其实就是一个神经元数量为V的一个输出层，最后要经过一个softmax分类器，把每个神经元的数量变成一个概率值，表示词典中对应位置的词是中心词（输入层的向量表示的词）附近的词的概率。

### 训练方法

* 训练数据

通过在训练文档中找到单词对来训练神经网络。下面的例子显示了一些训练样本（单词对），这些单词对将从“The quick brown fox jumps over the lazy dog.”获取。这里采用了一个小的窗口大小2,。标为高亮蓝色的是我们的输入单词。

![训练数据](/styles/imgs/NLP/skip-gram-example.jpg)


类似图中的例子，最终的输入样本就是一个(‘input’，‘output’)的单词对。

* 训练方式

**通过BP来学习权重**

使用比较常规的方式去训练模型，分下面2步

1. 定义损失函数.这个损失函数就是输出单词组的条件概率，一般都是取对数，具体如下：


![loos function](/styles/imgs/NLP/skip-gram-BP-lossfun1.png)

2. 对概率求导数并且使用梯度下降更新，具体计算略去，得到如下结果：

![weight derivative](/styles/imgs/NLP/skip-gram-lossfun2-weight.png)

![weight derivative](/styles/imgs/NLP/skip-gram-lossfun2-weight2.png)


### 一个简单的实现例子

